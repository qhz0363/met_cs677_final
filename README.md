# MET CS 677 Final Project
## Introduction
The purpose of my project is to build a chinese conversational artificial intelligence based on my own corpus.

- This project uses the GPT2 and transformers model to train Chinese gossip corpus.
- The logic of the decoder uses Temperature, Top-k Sampling and Nucleus Sampling, etc.
- According to the idea of Microsoft's DialoGPT, mutual information was added to the project. Two models are trained: Dialogue Model and MMI Model (maximum mutual information scoring function). First use the Dialogue Model to generate multiple candidate responses, and then use the MMI Model to select the least loss as the final response from the candidate responses.

## How to use
Put the downloaded model folders dialogue_model and mmi_model in the project root directory (otherwise you need to specify the path of the corresponding model through the --dialogue_model_path and --mmi_model_path parameters), and execute the following command:
### Without MMI
``` bash
python interact.py --no_cuda(Use the default parameters and do not use the GPU. Since the content generated by the small chat conversation is not long, the CPU can also be used at an acceptable speed)
or
python interact.py --no_cuda --dialogue_model_path path_to_dialogue_model --max_history_len 5(Custom --max_history_len parameter, that is, the length of the dialogue history)
or
python interact.py --no_cuda --dialogue_model_path path_to_dialogue_model --max_history_len 5 --topp 0.8 --topk 0(--topp is a decimal between 0 and 1, used to call Nucleus Sampling)
or
python interact.py --no_cuda --max_history_len 5 --topk 8(The --dialogue_model_path parameter is not specified, the default is dialogue_model)
``` 
### Use MMI
``` bash
python interact_mmi.py --no_cuda(使用默认的model路径)
或
python interact_mmi.py --no_cuda --batch_size 5(指定生成候选response的个数)
或
python interact_mmi.py --no_cuda --debug(debug模式，可以看到生成的所有候选response及其通过mmi_model的loss)
或
python interact_mmi.py --no_cuda --dialogue_model_path path_to_dialogue_model --mmi_model_path path_to_mmi_model(自定义模型路径)
```

## Training Model

If you are training mmi_model, you need to specify the --train_mmi parameter; if you are training dialogue_model, you do not need to specify the --train_mmi parameter
``` bash
# Training dialogue_model
python train.py --epochs 30 --batch_size 8 --device 0,1 --raw (If you want to tokenize the original training corpus, you must specify the --raw parameter. If you want to use GPU training, pass --device Specify GPU)
or
python train.py --epochs 30 --batch_size 8 --no_cuda --raw (Specify the --no_cuda parameter, use CPU training, the speed is much slower)
or
python train.py --epochs 30 --batch_size 8 --no_cuda (If you have already tokenized the original corpus, you don't need to specify --raw to avoid repeated tokenize and save time)
```
``` bash
#Training mmi_model, to specify the --train_mmi parameter  

python train.py --epochs 30 --batch_size 8 --device 0,1 --raw --train_mmi (reverse splicing of the original training corpus, tokenize, and train mmi_model)
or
python train.py --epochs 30 --batch_size 8 --device 0,1 --train_mmi (If the original training corpus has been tokenized, then directly train mmi_model)
or
python train.py --epochs 30 --batch_size 8 --device 0,1 --train_mmi --pretrained_model path_to_pretrained_model (continue training based on the training model)
```